{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftJ-T4KnQyhC"
      },
      "source": [
        "**Code to pull data from PT and save in Workday upload format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgmMme0ZMQGf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-CQvfmOQtp_"
      },
      "source": [
        "Installing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EciNG_JgxisL"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from pprint import pprint\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver import ActionChains\n",
        "from dateutil.parser import parse\n",
        "import pandas as pd\n",
        "import time\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQDJkh7SUmVJ"
      },
      "source": [
        "Pre-Run Checklist: \\\\\n",
        "Upload Mapping Master and Daily PT Data 2024 (linked template) \\\\\n",
        "Select which links to groups to run \\\\\n",
        "Define start and end date of run \\\\\n",
        "Check which locations sunday data is being included for, and which locations no\n",
        "data should be uploaded for \\\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMhhfhI9WgUt"
      },
      "outputs": [],
      "source": [
        "\n",
        "  # @title Enter PT User Details\n",
        "USR = input(\"Enter PT Username: \")\n",
        "PW = input(\"Enter PT Password: \")\n",
        "time.sleep(5)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFUrSRS_RwiM"
      },
      "source": [
        "Links to groups/practices to scrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4ZBK7qbsWD5"
      },
      "outputs": [],
      "source": [
        "# # means line is hidden (won't run)\n",
        "links = [\n",
        "        # ['Mark Chatham', 'https://app.performancetracker.co.uk/switch-group/52'],\n",
        "        #  ['Penny Goddard', 'https://app.performancetracker.co.uk/switch-group/27'],\n",
        "        #  ['Sarah Mendham', 'https://app.performancetracker.co.uk/switch-group/88'],\n",
        "        #  ['Damien Wallwork', 'https://app.performancetracker.co.uk/switch-group/82'],\n",
        "        #  ['David Thompson', 'https://app.performancetracker.co.uk/switch-group/81'],\n",
        "        #  ['Kim Whittet', 'https://app.performancetracker.co.uk/switch-group/65'],\n",
        "        #  ['Hannah Roberts', 'https://app.performancetracker.co.uk/switch-group/97'],\n",
        "        #  ['Dean Aldis', 'https://app.performancetracker.co.uk/switch-group/89'],\n",
        "        #  ['Fiona Halliday', 'https://app.performancetracker.co.uk/switch-group/95'],\n",
        "        #  ['Lynn Blacklaws', 'https://app.performancetracker.co.uk/groups/155'],\n",
        "        #  ['Paul Foxall', 'https://app.performancetracker.co.uk/switch-group/112'],\n",
        "        #  ['Angela Campbell', 'https://app.performancetracker.co.uk/switch-group/24'],\n",
        "        #  ['Chris Frost', 'https://app.performancetracker.co.uk/switch-group/140'],\n",
        "        #  ['Paul Mothershaw', 'https://app.performancetracker.co.uk/switch-group/26'],\n",
        "        #  ['Paul Forsythe', 'https://app.performancetracker.co.uk/switch-group/83'],\n",
        "        #  ['Hayley Gardner-Clark', 'https://app.performancetracker.co.uk/switch-group/29'],\n",
        "        #  ['Janine Andrews', 'https://app.performancetracker.co.uk/switch-group/122'],\n",
        "        #  ['Daniel Martin', 'https://app.performancetracker.co.uk/switch-group/121'],\n",
        "        #  ['Sally Whitworth', 'https://app.performancetracker.co.uk/switch-group/115'],\n",
        "        #  ['Stephen Potter', 'https://app.performancetracker.co.uk/switch-group/28'],\n",
        "        #  ['Richard Ogden', 'https://app.performancetracker.co.uk/switch-group/124'],\n",
        "        #  ['Margaret Mckergan', 'https://app.performancetracker.co.uk/switch-group/134'],\n",
        "        #  ['Nick Chindavata', 'https://app.performancetracker.co.uk/groups/39'],\n",
        "        # ['Trouteaud Opticians', 'https://app.performancetracker.co.uk/branches/519'],\n",
        "        ['Hakim Group', 'https://app.performancetracker.co.uk/groups/5'],\n",
        "        # ['Shephard & Akay', 'https://app.performancetracker.co.uk/switch-branch/303']\n",
        "        # ['Straine Opticians', 'https://app.performancetracker.co.uk/branches/539'],\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMKlKm3dR25-"
      },
      "source": [
        "Defining start/end date of scrape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select Weekly or Daily Data Pull\n",
        "time_delimeter = \"Weekly\" # @param [\"Weekly\",\"Daily\"]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S-pbcZDWy6WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdDBZvV0wMfJ",
        "outputId": "ec194e5b-a1ed-493d-ec13-a6ac73e5ed25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dates to be scraped:\n",
            "[datetime.date(2023, 1, 1), datetime.date(2023, 1, 8), datetime.date(2023, 1, 15), datetime.date(2023, 1, 22), datetime.date(2023, 1, 29), datetime.date(2023, 2, 5), datetime.date(2023, 2, 12), datetime.date(2023, 2, 19), datetime.date(2023, 2, 26), datetime.date(2023, 3, 5), datetime.date(2023, 3, 12), datetime.date(2023, 3, 19), datetime.date(2023, 3, 26), datetime.date(2023, 4, 2), datetime.date(2023, 4, 9), datetime.date(2023, 4, 16), datetime.date(2023, 4, 23), datetime.date(2023, 4, 30), datetime.date(2023, 5, 7), datetime.date(2023, 5, 14), datetime.date(2023, 5, 21), datetime.date(2023, 5, 28), datetime.date(2023, 6, 4), datetime.date(2023, 6, 11), datetime.date(2023, 6, 18), datetime.date(2023, 6, 25), datetime.date(2023, 7, 2), datetime.date(2023, 7, 9), datetime.date(2023, 7, 16), datetime.date(2023, 7, 23), datetime.date(2023, 7, 30), datetime.date(2023, 8, 6), datetime.date(2023, 8, 13), datetime.date(2023, 8, 20), datetime.date(2023, 8, 27), datetime.date(2023, 9, 3), datetime.date(2023, 9, 10), datetime.date(2023, 9, 17), datetime.date(2023, 9, 24), datetime.date(2023, 10, 1), datetime.date(2023, 10, 8), datetime.date(2023, 10, 15), datetime.date(2023, 10, 22), datetime.date(2023, 10, 29), datetime.date(2023, 11, 5), datetime.date(2023, 11, 12), datetime.date(2023, 11, 19), datetime.date(2023, 11, 26), datetime.date(2023, 12, 3), datetime.date(2023, 12, 10), datetime.date(2023, 12, 17), datetime.date(2023, 12, 24), datetime.date(2023, 12, 31)]\n"
          ]
        }
      ],
      "source": [
        "start_date = datetime.date(2023, 1, 1)  # date(year, month, date)\n",
        "end_date = datetime.date(2023, 12, 30)\n",
        "date = start_date\n",
        "dates = []\n",
        "if time_delimeter == \"Daily\":\n",
        "  datetime_delimeter = datetime.timedelta(days=1)\n",
        "if time_delimeter == \"Weekly\":\n",
        "  datetime_delimeter = datetime.timedelta(days=7)\n",
        "print(\"dates to be scraped:\")\n",
        "while date < end_date+datetime_delimeter:\n",
        "  dates.append(date)\n",
        "  # print(date)\n",
        "  date = date + datetime_delimeter\n",
        "print(dates)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dates[1].weekday())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K97zKJQc2Lr3",
        "outputId": "4501031a-b5d1-4d9f-9a82-f7ab6b257cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if time_delimeter == \"Daily\":\n",
        "  from_dates = dates\n",
        "  to_dates = dates\n",
        "if time_delimeter == \"Weekly\":\n",
        "  from_dates = []\n",
        "  to_dates = []\n",
        "  for date in dates:\n",
        "    week_monday = date-datetime.timedelta(days=date.weekday())\n",
        "    week_sunday = week_monday + datetime.timedelta(days=5)\n",
        "    from_dates.append(week_monday)\n",
        "    to_dates.append(week_sunday)\n",
        "\n",
        "print(\"from dates: \"+str(from_dates))\n",
        "print(\"to dates: \"+str(to_dates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13jQF1nK19yh",
        "outputId": "4ed65ff7-1beb-428e-e20b-fd3ca303c055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from dates: [datetime.date(2022, 12, 26), datetime.date(2023, 1, 2), datetime.date(2023, 1, 9), datetime.date(2023, 1, 16), datetime.date(2023, 1, 23), datetime.date(2023, 1, 30), datetime.date(2023, 2, 6), datetime.date(2023, 2, 13), datetime.date(2023, 2, 20), datetime.date(2023, 2, 27), datetime.date(2023, 3, 6), datetime.date(2023, 3, 13), datetime.date(2023, 3, 20), datetime.date(2023, 3, 27), datetime.date(2023, 4, 3), datetime.date(2023, 4, 10), datetime.date(2023, 4, 17), datetime.date(2023, 4, 24), datetime.date(2023, 5, 1), datetime.date(2023, 5, 8), datetime.date(2023, 5, 15), datetime.date(2023, 5, 22), datetime.date(2023, 5, 29), datetime.date(2023, 6, 5), datetime.date(2023, 6, 12), datetime.date(2023, 6, 19), datetime.date(2023, 6, 26), datetime.date(2023, 7, 3), datetime.date(2023, 7, 10), datetime.date(2023, 7, 17), datetime.date(2023, 7, 24), datetime.date(2023, 7, 31), datetime.date(2023, 8, 7), datetime.date(2023, 8, 14), datetime.date(2023, 8, 21), datetime.date(2023, 8, 28), datetime.date(2023, 9, 4), datetime.date(2023, 9, 11), datetime.date(2023, 9, 18), datetime.date(2023, 9, 25), datetime.date(2023, 10, 2), datetime.date(2023, 10, 9), datetime.date(2023, 10, 16), datetime.date(2023, 10, 23), datetime.date(2023, 10, 30), datetime.date(2023, 11, 6), datetime.date(2023, 11, 13), datetime.date(2023, 11, 20), datetime.date(2023, 11, 27), datetime.date(2023, 12, 4), datetime.date(2023, 12, 11), datetime.date(2023, 12, 18), datetime.date(2023, 12, 25)]\n",
            "to dates: [datetime.date(2022, 12, 31), datetime.date(2023, 1, 7), datetime.date(2023, 1, 14), datetime.date(2023, 1, 21), datetime.date(2023, 1, 28), datetime.date(2023, 2, 4), datetime.date(2023, 2, 11), datetime.date(2023, 2, 18), datetime.date(2023, 2, 25), datetime.date(2023, 3, 4), datetime.date(2023, 3, 11), datetime.date(2023, 3, 18), datetime.date(2023, 3, 25), datetime.date(2023, 4, 1), datetime.date(2023, 4, 8), datetime.date(2023, 4, 15), datetime.date(2023, 4, 22), datetime.date(2023, 4, 29), datetime.date(2023, 5, 6), datetime.date(2023, 5, 13), datetime.date(2023, 5, 20), datetime.date(2023, 5, 27), datetime.date(2023, 6, 3), datetime.date(2023, 6, 10), datetime.date(2023, 6, 17), datetime.date(2023, 6, 24), datetime.date(2023, 7, 1), datetime.date(2023, 7, 8), datetime.date(2023, 7, 15), datetime.date(2023, 7, 22), datetime.date(2023, 7, 29), datetime.date(2023, 8, 5), datetime.date(2023, 8, 12), datetime.date(2023, 8, 19), datetime.date(2023, 8, 26), datetime.date(2023, 9, 2), datetime.date(2023, 9, 9), datetime.date(2023, 9, 16), datetime.date(2023, 9, 23), datetime.date(2023, 9, 30), datetime.date(2023, 10, 7), datetime.date(2023, 10, 14), datetime.date(2023, 10, 21), datetime.date(2023, 10, 28), datetime.date(2023, 11, 4), datetime.date(2023, 11, 11), datetime.date(2023, 11, 18), datetime.date(2023, 11, 25), datetime.date(2023, 12, 2), datetime.date(2023, 12, 9), datetime.date(2023, 12, 16), datetime.date(2023, 12, 23), datetime.date(2023, 12, 30)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcfa8JoBVAwx"
      },
      "source": [
        "Defining which location to include sunday data for/exclude data from upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3yRe2L4VKbO"
      },
      "outputs": [],
      "source": [
        "include_sunday_data = [1149001,1168002]  # locations to include sunday data for\n",
        "remove_all_data = []  # locations to remove all data from upload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "072cnhS6VI1G"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4FA0bnzRnNQ"
      },
      "source": [
        "Defining functions to be used in scraping algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXYgM_53x4yW"
      },
      "outputs": [],
      "source": [
        "def login(username, password):  # go to and log into PT with login details\n",
        "  dr.get(\"https://app.performancetracker.co.uk/login\")\n",
        "  email_elmt = dr.find_element(By.ID, \"email\")\n",
        "  password_elmt = dr.find_element(By.ID, \"password\")\n",
        "  email_elmt.send_keys(username)\n",
        "  password_elmt.send_keys(password + Keys.ENTER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVEwO_4tykD1"
      },
      "outputs": [],
      "source": [
        "def generate_report():  # click generate report in PT\n",
        "  generate = dr.find_element(By.XPATH, '//button[@class=\"btn btn-primary btn-raised btn-block\"]')\n",
        "  generate.click()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjEmY9IysyLd"
      },
      "outputs": [],
      "source": [
        "def check_date(driver, target_date):  # cycle through date panel until month is correct\n",
        "  time.sleep(0.25)\n",
        "  next_month = dr.find_element(By.XPATH, '/html/body/div[3]/div/div/div[2]/div[1]/div[1]/button[2]')  # next month button\n",
        "  prev_month = dr.find_element(By.XPATH, '/html/body/div[3]/div/div/div[2]/div[1]/div[1]/button[1]')  # previous month button\n",
        "  time.sleep(0.25)\n",
        "  current_date = dr.find_element(By.XPATH, '/html/body/div[3]/div/div/div[2]/div[1]/div[1]/div/div/div')  # current date selection\n",
        "  print(\"Current month selection: \"+current_date.text)\n",
        "  curr_dt = parse(current_date.text)\n",
        "  # print(curr_dt)\n",
        "  if curr_dt.year < target_date.year:  # reqad current date and select previous or next month as appropriate\n",
        "    dr.execute_script(\"arguments[0].click();\", next_month)\n",
        "    return check_date(driver, target_date)\n",
        "\n",
        "  if curr_dt.year > target_date.year:\n",
        "      pri_month = dr.find_element(By.XPATH, '/html/body/div[3]/div/div/div[2]/div[1]/div[1]/button[1]')\n",
        "      dr.execute_script(\"arguments[0].click();\", pri_month)\n",
        "      return check_date(driver, target_date)\n",
        "\n",
        "  if curr_dt.month < target_date.month:\n",
        "      dr.execute_script(\"arguments[0].click();\", next_month)\n",
        "      return check_date(driver, target_date)\n",
        "\n",
        "  if curr_dt.month > target_date.month:\n",
        "      pri_month = dr.find_element(By.XPATH, '/html/body/div[3]/div/div/div[2]/div[1]/div[1]/button[1]')\n",
        "      dr.execute_script(\"arguments[0].click();\", pri_month)\n",
        "      return check_date(driver, target_date)\n",
        "\n",
        "  if curr_dt.month == target_date.month and curr_dt.year == target_date.year:  # if month is correct, continue\n",
        "    print('correct')\n",
        "    return\n",
        "  print('weird, continuing')\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgqHDcNzAx8y"
      },
      "outputs": [],
      "source": [
        "def select_date(driver, date):\n",
        "  # time.sleep(0.5)  # ensure button is ready to click\n",
        "  date_n = driver.find_elements(By.XPATH, \"//*[contains(text(), \" +str(date)+ \")]\")  # find all date buttons\n",
        "  for element in date_n:  # cycle through all date buttons\n",
        "    # print(element.text)\n",
        "    if element.text == str(date):  # if string in button matches target date save reference to button\n",
        "      date_element = element\n",
        "  date_element.text\n",
        "  driver.execute_script(\"arguments[0].click();\", date_element)  # click correct date button\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm57qTo0Sri8"
      },
      "source": [
        "Scrape PT data and save in PT Daily Data format as PT_Daily_Data_Scrape.xlsx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FID57bWvSJKH"
      },
      "outputs": [],
      "source": [
        "# dr.close\n",
        "options = webdriver.ChromeOptions()  # web driver setup options\n",
        "prefs = {'download.default_directory' : '/content/'}\n",
        "options.add_experimental_option('prefs', prefs)\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "dr = webdriver.Chrome(options=options)\n",
        "try:\n",
        "  login(USR,PW)\n",
        "except Exception as exc:\n",
        "  print('PT Login Failed, Error:')\n",
        "  print(exc)\n",
        "  print('Check Login Details are Correct')\n",
        "\n",
        "# date = first_monday  # start with first date\n",
        "with pd.ExcelWriter(\"PT_Daily_Data_Scrape.xlsx\") as writer:  # open daily scrape file to save data\n",
        "  for i in range(len(from_dates)):\n",
        "  # while date < final_sunday+datetime.timedelta(days=1):  # iterate from start to final date inclusive\n",
        "    # print(date)\n",
        "    try:\n",
        "      target_from_date = from_dates[i]\n",
        "    except Exception as exc:\n",
        "      print('PT Login Failed, Error:')\n",
        "      print(exc)\n",
        "      print('Check Login Details are Correct')\n",
        "      sys.exit()\n",
        "    target_to_date = to_dates[i]\n",
        "    print(target_from_date)\n",
        "    print(target_to_date)\n",
        "    all_groups_data = pd.DataFrame()\n",
        "    for group in links:  # if multiple groups selected iterate through each of them for each date\n",
        "      print(group[0])  # print group name\n",
        "      dr.get(group[1])  # go to group link\n",
        "      dr.get('https://app.performancetracker.co.uk/reports/180')  # go to Daily KPI and Sales report\n",
        "      # time.sleep(5)\n",
        "      try:\n",
        "        from_date = dr.find_element(By.NAME, 'from-date')  # try to find from date box, if not there it is likely that login details incorrect so scraper hasn't logged in\n",
        "      except Exception as exc:\n",
        "        print('PT Login Failed, Error:')\n",
        "        print(exc)\n",
        "        print('Check Login Details are Correct')\n",
        "        quit()\n",
        "        time.sleep(10)\n",
        "      # from_date = dr.find_element(By.NAME, 'from-date')\n",
        "      to_date = dr.find_element(By.NAME, 'to-date')  # find to date box\n",
        "\n",
        "      print('to date')\n",
        "      dr.execute_script(\"arguments[0].click();\", to_date)  # click to date box to open date selector\n",
        "      try:\n",
        "        check_date(dr, target_to_date)  # run algorithm to select correct month and year, if it doesn't work at this stage it may be due to a popup\n",
        "      except:\n",
        "        print('exception reached')\n",
        "        # dr.execute_script(\"arguments[0].click();\", clear_notifications)\n",
        "        dr.execute_script(\"arguments[0].click();\", dr.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/header/div[2]/div/div/div/button'))  # close popup\n",
        "        dr.execute_script(\"arguments[0].click();\", dr.find_element(By.XPATH, '/html/body/div/div/div[3]/header/div[1]/div/a')) # x new reports\n",
        "        dr.execute_script(\"arguments[0].click();\", dr.find_element(By.XPATH, '/html/body/div/div/div[3]/header/div[1]/div/ul/div[1]'))\n",
        "      select_date(dr,target_to_date.day)  # click correct date\n",
        "      ok_button = dr.find_element(By.XPATH, '/html/body/div[3]/div/div/div[2]/div[2]/button[2]')  # find ok button\n",
        "      dr.execute_script(\"arguments[0].click();\", ok_button)  # click ok button\n",
        "\n",
        "\n",
        "\n",
        "      print('from date')\n",
        "      dr.execute_script(\"arguments[0].click();\", from_date)  # open from date box\n",
        "      check_date(dr, target_from_date)  # correct month\n",
        "      select_date(dr,target_from_date.day)  # select date\n",
        "      ok_button = dr.find_element(By.XPATH, '/html/body/div[3]/div/div/div[2]/div[2]/button[2]')  # find new ok button\n",
        "      dr.execute_script(\"arguments[0].click();\", ok_button)  # click ok button\n",
        "\n",
        "\n",
        "\n",
        "      generate_report()  # click generate report\n",
        "      hidden_elements = np.zeros(4)\n",
        "      slept_time = 0\n",
        "      while len(hidden_elements) < 5:  # once report has loaded there will be a new hidden element\n",
        "        # print('waiting')\n",
        "        time.sleep(5)\n",
        "        slept_time = slept_time + 5\n",
        "        soup = BeautifulSoup(dr.page_source)  # parse page source\n",
        "        hidden_elements = soup(text=re.compile('react-empty:'))  # find hidden elements\n",
        "        print('Waited '+str(slept_time)+'s, report not yet loaded')  # no. hidden elmts: '+str(len(hidden_elements)))\n",
        "      print('now loaded, getting data')\n",
        "\n",
        "      download = dr.find_element(By.XPATH, \"//*[contains(text(), 'Download CSV')]\")  # find download csv button\n",
        "      download.click()  # click download csv\n",
        "\n",
        "      downloaded = 0\n",
        "      # iterations = 0\n",
        "      while downloaded < 1:  # check whether download complete\n",
        "        # iterations  = iterations+1\n",
        "        # print(iterations)\n",
        "        for file in os.listdir('/content/'):\n",
        "          # print(iterations)\n",
        "          if \"download\" in file:\n",
        "            print(file)\n",
        "            downloaded = 1  # if file \"download\"\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "      for file in os.listdir('/content/'):\n",
        "        if \"download\" in file:\n",
        "          print(file)\n",
        "          daily_data = pd.read_csv('/content/'+file, skiprows=1)  # read downloaded file\n",
        "          time.sleep(1)\n",
        "          os.remove('/content/'+file)\n",
        "\n",
        "      if len(all_groups_data) == 0:  # if there is already data from another group saved, merge data, else keep single group's file\n",
        "        all_groups_data = daily_data\n",
        "      else:\n",
        "        all_groups_data = pd.merge(all_groups_data, daily_data, on=\"Metrics\")\n",
        "\n",
        "    all_groups_data.to_excel(writer, sheet_name=target_from_date.strftime('%d.%m.%Y'), index=False)  # once iterated for all groups for a date, save the data in excel file with date as sheet name\n",
        "    # date = date + datetime.timedelta(days=1)  # next date\n",
        "\n",
        "dr.close"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe9uIIKNTRgl"
      },
      "source": [
        "PT to Workday Account Names Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5953aByEw2l"
      },
      "outputs": [],
      "source": [
        "account_codes = {\"Sight Tests\": \"Total Sight Tests\",  # {Workday Account name: PT Account name}\n",
        "                \"ADV\": \"Average Dispense Value (ADV)\",\n",
        "                \"Cost of Sales\": \"Supplier Invoices (ex VAT)\",\n",
        "                \"Gross Margin\": \"Gross Margin\",\n",
        "                 \"Optom DER %\": \"Optom DER (%)\",\n",
        "                 \"Practice DER %\": \"Practice DER (%)\",\n",
        "                 \"Turnover Inc VAT\": \"Turnover (inc VAT)\",\n",
        "                 \"Turnover Ex VAT\": \"Turnover (ex VAT)\",\n",
        "                 \"Available Appointments\": \"Available Appointments\",\n",
        "                 \"Booked Appointments\": \"Total Appointments Booked\",\n",
        "                 \"New Patients\": \"Number of New Patients\",\n",
        "                 \"Refunds\": \"Refunds\",\n",
        "                 \"FTA\": \"FTA\",\n",
        "                 \"Optom Number of Dispenses\": \"How Many Advised Specs\",\n",
        "                 \"Number of Dispenses\": \"How Many Dispensed Specs\",\n",
        "                 \"Value of Dispenses\": \"Total Specs Sales\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiFLDb1dTbb9"
      },
      "source": [
        "Get List of PT Practices from Daily PT Data 2024 (linked template).xlsx (ensure location codes/names in columns C/D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kayzThEZCXVD"
      },
      "outputs": [],
      "source": [
        "pt_practices = pd.read_excel('/content/Historic PT Scrape Codes.xlsx',sheet_name='Sheet 1',usecols='C,D')\n",
        "pt_practice_codes = pt_practices['Unnamed: 2'].dropna()  # drop blanks/errors\n",
        "pt_practice_codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epzbaz92T6kK"
      },
      "source": [
        "Convert Scraped data saved in PT Daily Data format to Workday upload format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSjI2ludFUIs"
      },
      "outputs": [],
      "source": [
        "mapping_master = pd.read_excel('/content/Mapping Master.xlsx')  # read mapping master to get PT Name associated with each Workday Location Code\n",
        "\n",
        "# date = first_monday\n",
        "locations_not_in_PT_export = []\n",
        "weekly_upload_df = pd.DataFrame()\n",
        "# while date < final_sunday+datetime.timedelta(days=1):  # for each date\n",
        "for i in range(len(from_dates)):\n",
        "  date = from_dates[i]\n",
        "  print(date)\n",
        "  daily_upload_df = pd.DataFrame()  # blank daily dataframe\n",
        "  day_data = pd.read_excel('/content/PT_Daily_Data_Scrape.xlsx',sheet_name=date.strftime('%d.%m.%Y'),index_col=0)  # ,skiprows=1  # read PT data for date from saved excel sheet\n",
        "  data_with_codes = day_data.transpose().join(mapping_master[['PT 2023 Name','Workday Entity Code','Workday Location Code']].set_index('PT 2023 Name'))  # add location codes to pt data sheet\n",
        "  for location in pt_practice_codes:  # for each location code  # switched from daily pt scraper\n",
        "    for account in account_codes.keys():  # for each account\n",
        "      if len(data_with_codes.loc[data_with_codes['Workday Location Code'] == location][account_codes[account]]) == 0:  # if there is no data found for this location and account\n",
        "        if location not in locations_not_in_PT_export:  # and it hasn't already been added to the list of locations without data\n",
        "          locations_not_in_PT_export.append(location)  # add to list\n",
        "      daily_upload_df = pd.concat([daily_upload_df, pd.DataFrame({\"Account\": account,  # add line to dataframe with account, level code, location code, and data\n",
        "                                    \"Level Code\": data_with_codes.loc[data_with_codes['Workday Location Code'] == location][\"Workday Entity Code\"],\n",
        "                                    \"Location Code\": data_with_codes.loc[data_with_codes['Workday Location Code'] == location][\"Workday Location Code\"],\n",
        "                                    date.strftime('%d/%m/%Y'): data_with_codes.loc[data_with_codes['Workday Location Code'] == location][account_codes[account]]})])  # data column name is the date\n",
        "\n",
        "\n",
        "\n",
        "  if len(weekly_upload_df) == 0:  # if no data in weekly upload sheet yet then current day is full sheet\n",
        "    weekly_upload_df = daily_upload_df\n",
        "  else:  # merge current days data with previous days, merge on account and location code, level code doesn't need adding again\n",
        "    weekly_upload_df = pd.merge(weekly_upload_df, daily_upload_df.drop(columns=[\"Level Code\"]), how='left', left_on=[\"Account\", \"Location Code\"], right_on=[\"Account\", \"Location Code\"])\n",
        "  date = date + datetime.timedelta(days=1)  # next date\n",
        "\n",
        "print('Locations not in PT Export:')\n",
        "print(locations_not_in_PT_export)  # print missing locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuDkG4GiMWQX"
      },
      "source": [
        "Locations in uploaded list but not in PT Export:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBheCCguMZaP"
      },
      "outputs": [],
      "source": [
        "print(locations_not_in_PT_export)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raTV1MA8UD0M"
      },
      "source": [
        "Remove Sunday data for all practices apart from exceptions, and remove all data for practices in list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmyMu_Cr6MD3"
      },
      "outputs": [],
      "source": [
        "# # date = first_monday\n",
        "# # while date < final_sunday+datetime.timedelta(days=1):  #\n",
        "# for i in range(len(from_dates)):\n",
        "#   date = from_dates[i]\n",
        "#   weekly_upload_df.loc[weekly_upload_df['Location Code'].isin(remove_all_data) , date.strftime('%d/%m/%Y')] = 0  # remove all data for locations in \"remove_all_data\" list\n",
        "#   if date.weekday() == 6:\n",
        "#     weekly_upload_df.loc[~weekly_upload_df['Location Code'].isin(include_sunday_data) , date.strftime('%d/%m/%Y')] = 0  # include sunday data for locations in \"include_sunday_data\"\n",
        "#   date = date + datetime.timedelta(days=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMR87F7UUZMu"
      },
      "source": [
        "Save as curr_week_daily_PT_data.xlsx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beNjML3reBO4"
      },
      "outputs": [],
      "source": [
        "weekly_upload_df.to_excel(\"PT_Weekly_Data_WD_Upload.xlsx\", index=False)  # save week's workday data\n",
        "weekly_upload_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
